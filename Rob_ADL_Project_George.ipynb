{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rob - ADL_Project_George.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mishless/heroes-game-planning/blob/master/Rob_ADL_Project_George.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO9yVEAiJ-X0",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oDfQ8YnKAS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "\n",
        "from datetime import datetime\n",
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import CIFAR10, CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import argparse\n",
        "from copy import deepcopy\n",
        "from tqdm import trange\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cGy0RLjJY0x",
        "colab_type": "text"
      },
      "source": [
        "# Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsR2ifaJJU8S",
        "colab_type": "code",
        "outputId": "6f6f1c69-7d9a-487e-df01-6313d3f9fbe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive\")\n",
        "main_path = os.path.join(\"My Drive\", \"ADL_Project\")\n",
        "if not os.path.exists(main_path):\n",
        "    os.mkdir(main_path)\n",
        "os.chdir(main_path)\n",
        "os.getcwd()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/ADL_Project'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDDmCfbi515D",
        "colab_type": "code",
        "outputId": "332837be-cd2e-4573-9f09-d7acb0b46ba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "os.listdir(\".\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Data',\n",
              " 'mwn',\n",
              " 'meta_weight_net',\n",
              " 'meta_model',\n",
              " 'model',\n",
              " 'Experiments',\n",
              " 'CIFAR_10_Uniform noise_0_MWN',\n",
              " 'CIFAR_10_Uniform noise_0.4_MWN',\n",
              " 'CIFAR_10_Uniform noise_0.4_MWN_confusion_matrix.png',\n",
              " 'CIFAR_10_Imbalance_200_MWN']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIbc2CZMLp1R",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onIKtBfO1YWb",
        "colab_type": "text"
      },
      "source": [
        "## Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFfVtM-n1Z_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                     std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: F.pad(x.unsqueeze(0),\n",
        "                                      (4, 4, 4, 4), mode='reflect').squeeze()),\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0RVVIa11a5G",
        "colab_type": "text"
      },
      "source": [
        "## Prepare data (download)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUThwFCG1evV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_CIFAR_data(cifar=10, train_transform=None, test_transform=None,\n",
        "                   train_target_transform=None, test_target_transform=None):\n",
        "    \n",
        "    if cifar == 10:\n",
        "        dataset = CIFAR10\n",
        "        path = os.path.join(\"Data\", \"cifar-10-batches-py\")\n",
        "    elif cifar == 100:\n",
        "        dataset = CIFAR100\n",
        "        path = os.path.join(\"Data\", \"cifar-100-python\")\n",
        "                \n",
        "    train_data = dataset(path, train=True,\n",
        "                         transform=train_transform, \n",
        "                         target_transform=train_target_transform, \n",
        "                         download= not os.path.exists(path))\n",
        "        \n",
        "    test_data = dataset(path, train=False, \n",
        "                        transform=test_transform, \n",
        "                        target_transform=test_target_transform, \n",
        "                        download= not os.path.exists(path))\n",
        "    \n",
        "    return train_data, test_data\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSFylcEo1nPG",
        "colab_type": "text"
      },
      "source": [
        "## Creating imbalanced data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "288tDpq01qPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_imbalance_data(dataset, \n",
        "                            factor=200, \n",
        "                            num_meta_per_class=10, \n",
        "                            num_of_corrupted=10,\n",
        "                            seed=123):\n",
        "    imbalance_factor = factor\n",
        "    \n",
        "    num_of_classes = np.unique(dataset.targets).shape[0]\n",
        "    num_of_total_targets = len(dataset.targets)\n",
        "    \n",
        "    indices_dict = {target: np.where(np.array(dataset.targets) == target)[0] \n",
        "                    for target in range(num_of_classes)}\n",
        "    \n",
        "    meta_indices = []\n",
        "    train_indices = []\n",
        "    np.random.seed(seed)\n",
        "    for target in indices_dict.keys():\n",
        "        np.random.shuffle(indices_dict[target])\n",
        "        meta_indices += indices_dict[target][: num_meta_per_class].tolist()\n",
        "        \n",
        "        imbalance_size = int((\n",
        "            indices_dict[target].shape[0] - num_meta_per_class) / float(\n",
        "            imbalance_factor) ** float(target / (num_of_classes-1) ) )\n",
        "        \n",
        "        train_indices +=  indices_dict[target][\n",
        "            num_meta_per_class: num_meta_per_class + imbalance_size].tolist()\n",
        "        \n",
        "    train_data = deepcopy(dataset)\n",
        "    meta_data = deepcopy(dataset)\n",
        "    \n",
        "    train_indices = np.array(train_indices).flatten()\n",
        "    meta_indices = np.array(meta_indices).flatten()    \n",
        "    \n",
        "    train_data.data = train_data.data[train_indices]\n",
        "    meta_data.data = meta_data.data[meta_indices]\n",
        "    \n",
        "    train_data.targets = np.array(train_data.targets)[train_indices]\n",
        "    meta_data.targets = np.array(meta_data.targets)[meta_indices]\n",
        "       \n",
        "    return train_data, meta_data, None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmRVQlHWflgf",
        "colab_type": "text"
      },
      "source": [
        "## Uniform noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8i500Mqfk0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_noise_data(dataset, \n",
        "                        factor=0.4, \n",
        "                        num_meta_per_class=10, \n",
        "                        num_of_corrupted=10,\n",
        "                        seed=123):\n",
        "\n",
        "    noise_factor = factor\n",
        "\n",
        "    num_of_classes = np.unique(dataset.targets).shape[0]\n",
        "    num_of_total_targets = len(dataset.targets)\n",
        "\n",
        "    indices_array = np.array([np.where(np.array(dataset.targets) == target)[0] \n",
        "                          for target in range(num_of_classes)])\n",
        "\n",
        "    temp_indices = np.empty((num_meta_per_class, indices_array.shape[1] - num_meta_per_class))\n",
        "    meta_indices = []\n",
        "    np.random.seed(seed)\n",
        "    for n in range(num_of_classes):\n",
        "        np.random.shuffle(indices_array[n])\n",
        "        meta_indices.append(indices_array[n, : num_meta_per_class])\n",
        "        temp_indices[n] = indices_array[n, num_meta_per_class: ]\n",
        "\n",
        "    train_data = deepcopy(dataset)\n",
        "    meta_data = deepcopy(dataset)\n",
        "\n",
        "    train_indices = temp_indices.flatten().astype(int)\n",
        "    np.random.shuffle(train_indices)\n",
        "    meta_indices = np.array(meta_indices).flatten().astype(int)\n",
        "\n",
        "    train_data.data = train_data.data[train_indices]\n",
        "    meta_data.data = meta_data.data[meta_indices]\n",
        "\n",
        "    train_data.targets = np.array(train_data.targets)[train_indices]\n",
        "    meta_data.targets = np.array(meta_data.targets)[meta_indices]\n",
        "\n",
        "    corrupted_indices = []\n",
        "    chosen_indices = [[x for x in range(num_of_classes) if x != t] for t in range(num_of_classes)]\n",
        "    for i in range(num_of_classes):\n",
        "      target_indices = np.where(train_data.targets == i)[0]\n",
        "      for n in target_indices:\n",
        "        if np.random.random() < factor:\n",
        "          train_data.targets[n] = np.random.choice(chosen_indices[i], 1)\n",
        "          corrupted_indices.append(n)\n",
        "\n",
        "    np.random.shuffle(corrupted_indices)\n",
        "    mask = np.full(train_data.targets.shape[0], False)\n",
        "    mask[corrupted_indices[:num_of_corrupted]] = True\n",
        "\n",
        "    corrupted_data = deepcopy(train_data)\n",
        "\n",
        "    corrupted_data.data = deepcopy(train_data.data[mask])\n",
        "    corrupted_data.targets = deepcopy(train_data.targets[mask])\n",
        "    \n",
        "    return train_data, meta_data, corrupted_data\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khQzoQcxqzlH",
        "colab_type": "text"
      },
      "source": [
        "## Flip noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NVnYYMHqxcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_flip_data(dataset, \n",
        "                       factor=0.4, \n",
        "                       num_meta_per_class=10, \n",
        "                       num_of_corrupted=10,\n",
        "                       seed=123):\n",
        "\n",
        "  num_of_classes = np.unique(dataset.targets).shape[0]\n",
        "\n",
        "  indices_array = np.array([np.where(np.array(dataset.targets) == target)[0] \n",
        "                        for target in range(num_of_classes)])\n",
        "\n",
        "  temp_indices = np.empty((num_meta_per_class, indices_array.shape[1] - num_meta_per_class))\n",
        "  meta_indices = []\n",
        "  np.random.seed(seed)\n",
        "  for n in range(num_of_classes):\n",
        "      np.random.shuffle(indices_array[n])\n",
        "      meta_indices.append(indices_array[n, : num_meta_per_class])\n",
        "      temp_indices[n] = indices_array[n, num_meta_per_class: ]\n",
        "\n",
        "  train_data = deepcopy(dataset)\n",
        "  meta_data = deepcopy(dataset)\n",
        "\n",
        "  train_indices = temp_indices.flatten().astype(int)\n",
        "  np.random.shuffle(train_indices)\n",
        "  meta_indices = np.array(meta_indices).flatten().astype(int)\n",
        "\n",
        "  train_data.data = train_data.data[train_indices]\n",
        "  meta_data.data = meta_data.data[meta_indices]\n",
        "\n",
        "  train_data.targets = np.array(train_data.targets)[train_indices]\n",
        "  meta_data.targets = np.array(meta_data.targets)[meta_indices]\n",
        "\n",
        "\n",
        "  classes = list(range(num_of_classes))\n",
        "  np.random.shuffle(classes)\n",
        "\n",
        "  random_pair_classes = [[i, j] for i, j in zip(\n",
        "      classes[: int(len(classes)/2)], \n",
        "      classes[int(len(classes)/2): ])]\n",
        "\n",
        "  corrupted_indices = []\n",
        "  for pair in random_pair_classes:\n",
        "    target_indices_0 = np.where(train_data.targets == pair[0])[0]\n",
        "    np.random.shuffle(target_indices_0)\n",
        "    target_indices_1 = np.where(train_data.targets == pair[1])[0]\n",
        "    np.random.shuffle(target_indices_1)\n",
        "\n",
        "    size = int(target_indices_0.shape[0] * factor)\n",
        "\n",
        "    train_data.targets[target_indices_0][: size] = pair[1]\n",
        "    train_data.targets[target_indices_1][: size] = pair[0]\n",
        "\n",
        "    corrupted_indices += target_indices_0.tolist() + target_indices_1.tolist()  \n",
        "\n",
        "  np.random.shuffle(corrupted_indices)\n",
        "  mask = np.full(train_data.targets.shape[0], False)\n",
        "  mask[corrupted_indices[:num_of_corrupted]] = True\n",
        "\n",
        "  corrupted_data = deepcopy(train_data)\n",
        "\n",
        "  corrupted_data.data = deepcopy(train_data.data[mask])\n",
        "  corrupted_data.targets = deepcopy(train_data.targets[mask])\n",
        "       \n",
        "  return train_data, meta_data, corrupted_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSVAlk9XfoQl",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLhYaI52fni1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VNCY0kqKcVZ",
        "colab_type": "text"
      },
      "source": [
        "# Models - Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhq0pv4H1Jf7",
        "colab_type": "text"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZVzAoOn1I9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim=1, hidden_dim=100, output_dim=1, \n",
        "                 activation_hidden=torch.relu, activation_output=torch.sigmoid,\n",
        "                 initialization=nn.init.kaiming_normal_,\n",
        "                 mu=None, std=None, bias=True):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.activation_hidden = activation_hidden\n",
        "        self.activation_output = activation_output\n",
        "        \n",
        "        self.first_layer = nn.Linear(input_dim, hidden_dim, bias=bias)\n",
        "        self.second_layer = nn.Linear(hidden_dim, output_dim, bias=bias)\n",
        "\n",
        "        if initialization == nn.init.kaiming_normal_:\n",
        "            initialization(self.first_layer.weight)\n",
        "            initialization(self.second_layer.weight)\n",
        "        elif initialization == nn.init.normal_:\n",
        "            mu = 0.0 if mu is None else mu\n",
        "            std = 1.0 if std is None else std\n",
        "            initialization(self.first_layer.weight, mu, std)\n",
        "            initialization(self.second_layer.weight, mu, std)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        self.first_layer_output = self.activation_hidden(self.first_layer(x))\n",
        "        return self.activation_output(self.second_layer(self.first_layer_output))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxw6xoJsKfAq",
        "colab_type": "text"
      },
      "source": [
        "## Resnet-32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-td7pekIKecb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "      \n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.fc = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def resnet32():\n",
        "    return ResNet(BasicBlock, num_blocks=[3,4,5,3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9-BgMzLKhS4",
        "colab_type": "text"
      },
      "source": [
        "## Wide-Resnet-28-10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLeaErurKpsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x1F8p2extla",
        "colab_type": "text"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX0PLtKdKLmP",
        "colab_type": "text"
      },
      "source": [
        "## Meta-training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYWXdqewxxti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def meta_training(args, train_loader, meta_loader, model, meta_model, meta_weight_net, optimizers, loss_functions):\n",
        "  \n",
        "  def normalize_weights(weights, args):\n",
        "    sum_weights = torch.sum(weights)\n",
        "    denom = sum_weights if sum_weights != 0 else args.tau\n",
        "    return (weights / denom).to(args.cuda)\n",
        " \n",
        "  train_loss = 0\n",
        "  train_loss_weighted = 0\n",
        "  meta_loss = 0\n",
        "  train_predictions = []\n",
        "  train_targets = []\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  #for (x, y), (x_meta, y_meta) in zip(train_loader, meta_loader):\n",
        "  \n",
        "  for enum, (x, y) in tqdm(enumerate(train_loader)):\n",
        "#    if enum == 3:\n",
        "#      break\n",
        "    x_meta, y_meta = next(iter(meta_loader))\n",
        "#    print('batch nr: ',enum)\n",
        "#    if args.cuda == \"cuda:0\":\n",
        "#      torch.cuda.empty_cache()\n",
        "#    x.float(), y.float(), x_meta.float(), y_meta.float()\n",
        "    x, y, x_meta, y_meta = x.to(args.cuda), y.to(args.cuda), x_meta.to(args.cuda), y_meta.to(args.cuda)\n",
        "\n",
        "    # forward pass for meta-model using input data\n",
        "    y_pred = meta_model(x)\n",
        "    loss = loss_functions['model'](y_pred, y)\n",
        "\n",
        "    # forward pass for meta-weight-net\n",
        "    weights = meta_weight_net(loss.reshape(-1,1))\n",
        "    \n",
        "    # normalize weights and get weighted loss\n",
        "    normalized_weights = normalize_weights(weights, args)\n",
        "    weighted_loss = torch.mean(loss * normalized_weights)\n",
        "\n",
        "    # backward pass for meta-model\n",
        "    meta_model.zero_grad()\n",
        "    weighted_loss.backward()\n",
        "    \n",
        "    # update parameters of meta-model\n",
        "    optimizers['meta_model'].step()\n",
        "    \n",
        "\n",
        "    # ----- STEP 6 ----- #\n",
        "    # forward pass for meta model using meta data\n",
        "    y_meta_pred = meta_model(x_meta)\n",
        "    loss_meta = torch.mean(loss_functions['model'](y_meta_pred, y_meta))\n",
        "\n",
        "    meta_loss += loss_meta\n",
        "    \n",
        "    # backward pass for meta weight net\n",
        "    meta_weight_net.zero_grad()\n",
        "    loss_meta.backward()\n",
        "    \n",
        "    # update parameters of meta weight net\n",
        "    optimizers['meta_weight_net'].step()\n",
        "    \n",
        "\n",
        "    # ----- STEP 7 ----- #\n",
        "    # forward pass for model using input data\n",
        "    y_pred = model(x)\n",
        "\n",
        "    train_predictions.append(torch.argmax(y_pred,dim=1))\n",
        "    train_targets.append(y)\n",
        "    \n",
        "    loss = loss_functions['model'](y_pred, y)\n",
        "\n",
        "    # forward pass for updated meta-weight-net\n",
        "    weights = meta_weight_net(loss.reshape(-1,1))\n",
        "    \n",
        "    # normalize weights \n",
        "    normalized_weights = normalize_weights(weights, args)\n",
        "    \n",
        "    # compute weighted loss\n",
        "    weighted_loss = torch.mean(loss * normalized_weights)\n",
        "    \n",
        "    train_loss += torch.mean(loss)\n",
        "    train_loss_weighted += weighted_loss.item()\n",
        "    \n",
        "    # backward pass for model\n",
        "    model.zero_grad()\n",
        "    weighted_loss.backward()\n",
        "    \n",
        "    # update parameters of model\n",
        "    optimizers['model'].step()\n",
        "    #meta_model = deepcopy(model)\n",
        "    meta_model.load_state_dict(model.state_dict())\n",
        "    \n",
        "  train_loss /= len(train_loader)          # old comment: loss function already averages over batch size?\n",
        "  train_loss_weighted /= len(train_loader) # old comment: loss function already averages over batch size?\n",
        "  meta_loss /= len(train_loader)\n",
        "  \n",
        "  return model, meta_model, meta_weight_net, train_loss, train_loss_weighted, meta_loss, train_predictions, train_targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZraAtrdJpPk",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCsK--R8JnZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(args, data_loader, model, loss_function):\n",
        "    \n",
        "    predictions = []\n",
        "    targets = []\n",
        "\n",
        "    evaluation_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      for x_eval, y_eval in data_loader:\n",
        "\n",
        "   #       if args.cuda == \"cuda:0\":\n",
        "   #         torch.cuda.empty_cache()\n",
        "\n",
        "          #  x_eval.float(), y_eval.float()\n",
        "          x_eval, y_eval = x_eval.to(args.cuda), y_eval.to(args.cuda)\n",
        "\n",
        "          # forward pass\n",
        "          y_pred = model(x_eval)\n",
        "\n",
        "          predictions.append(torch.argmax(y_pred,dim=1))\n",
        "          targets.append(y_eval)\n",
        "\n",
        "          # compute loss\n",
        "          loss = loss_function(y_pred, y_eval)\n",
        "          #print(loss.cpu())\n",
        "          evaluation_loss += torch.mean(loss)\n",
        "\n",
        "    evaluation_loss /= len(data_loader)\n",
        "    return evaluation_loss, predictions, targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg-EkhWO-GxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weight_variation(args, w_previous, corrupted_data_loader, model, meta_weight_net, model_loss_function):\n",
        "  \n",
        "  x, y_noisy = next(iter(corrupted_data_loader))\n",
        "  \n",
        "#  if args.cuda == 'cuda:0':\n",
        "#    torch.cuda.empty_cache()\n",
        "  \n",
        "  x, y_noisy = x.to(args.cuda), y_noisy.to(args.cuda)\n",
        "  \n",
        "  y_pred = model(x)\n",
        "  loss = model_loss_function(y_pred, y_noisy)\n",
        "  w = meta_weight_net(loss.reshape(-1,1))\n",
        "  \n",
        "  w_variation = w - w_previous\n",
        "  w_variation_mean = Variable(torch.mean(w_variation)).cpu().numpy()\n",
        "  w_variation_std = Variable(torch.std(w_variation)).cpu().numpy()\n",
        "  \n",
        "  return w, w_variation_mean, w_variation_std\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1dhOZV8GE1e",
        "colab_type": "text"
      },
      "source": [
        "## Perform experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WPIjTtZGESJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perform_experiment(args, train_loader, meta_loader, test_loader, corrupted_data_loader, model, meta_model, meta_weight_net, optimizers, loss_functions):\n",
        "    \n",
        "  def compute_accuracy(torch_predictions, torch_targets):\n",
        "    predictions = Variable(torch.cat(torch_predictions)).cpu().numpy()\n",
        "    targets = Variable(torch.cat(torch_targets)).cpu().numpy()\n",
        "    accuracy = np.sum(np.where(targets == predictions,1,0)) / targets.size\n",
        "    return accuracy\n",
        "  \n",
        "  history = {'train_losses': [],\n",
        "             'train_losses_weighted': [],\n",
        "             'meta_losses': [],\n",
        "             'test_losses': [],\n",
        "             'train_accuracy': [],\n",
        "             'test_accuracy': [],\n",
        "             'test_targets': []}\n",
        "  \n",
        "  best_test_model_loss = np.inf\n",
        "  best_model_info = {}\n",
        "  best_test_model = None\n",
        "  best_test_model_predictions = None\n",
        "  best_test_model_targets = None\n",
        "  best_test_model_accuracy = None\n",
        "  \n",
        "  if args.experiment_type == \"Uniform noise\": # ONLY FOR UNIFORM\n",
        "    w_previous = 0\n",
        "    history['weight_variation_means'] = []\n",
        "    history['weight_variation_stds'] = []\n",
        "  \n",
        "  cifar_type = str(args.cifar_type) if args.dataset == 'CIFAR' else \"\"\n",
        "  path = os.path.join(args.directory, args.dataset + cifar_type)\n",
        "  path = args.directory\n",
        "  \n",
        "  epochs = trange(args.epochs_dic[args.experiment_type], leave=True)\n",
        "  for epoch in epochs:\n",
        "        \n",
        "    if epoch in args.lr_schedule:\n",
        "      optimizers['model'].param_groups[0]['lr'] = args.lr_schedule[epoch]\n",
        "    \n",
        "    time_start = time.time()\n",
        "    \n",
        "    # training\n",
        "    model, meta_model, meta_weight_net, train_loss, train_loss_weighted, meta_loss, train_predictions, train_targets = meta_training(args, train_loader, meta_loader,  model, meta_model, meta_weight_net, optimizers, loss_functions) #args\n",
        "    train_accuracy = compute_accuracy(train_predictions, train_targets)\n",
        "    \n",
        "    #print(np.unique(torch.cat(train_targets).cpu().numpy(),return_counts=True))\n",
        "\n",
        "    # evaluation on test set\n",
        "    test_loss, test_predictions, test_targets = evaluate(args, test_loader, model, loss_functions['model'])\n",
        "    test_accuracy = compute_accuracy(test_predictions, test_targets)\n",
        "   \n",
        "    # weight variation\n",
        "    if args.experiment_type == \"Uniform noise\":\n",
        "      w, w_variation_mean, w_variation_std = weight_variation(args, w_previous, corrupted_data_loader, model, meta_weight_net, loss_functions['model'])\n",
        "      history['weight_variation_means'].append(w_variation_mean)\n",
        "      history['weight_variation_stds'].append(w_variation_std)\n",
        "      w_previous = w\n",
        "  \n",
        "    time_for_epoch = time.time() - time_start\n",
        "    \n",
        "    def rounding(tensor,decimals):\n",
        "      return torch.round(tensor * 10**decimals) / (10**decimals)\n",
        "    \n",
        "    # update progress bar\n",
        "    epochs.set_description(\"Time for epoch: {}, \\\n",
        "Train loss: {}, \\\n",
        "Train loss weighted: {}, \\\n",
        "Meta loss: {}, \\\n",
        "Test loss: {}\".format(time_for_epoch\n",
        "                     ,rounding(train_loss,5)\n",
        "                     ,round(train_loss_weighted,5)\n",
        "                     ,rounding(meta_loss,5)\n",
        "                     ,rounding(test_loss,5)\n",
        "                     ))\n",
        "                           \n",
        "    if test_loss < best_test_model_loss:\n",
        "      best_test_model = model\n",
        "      best_test_model_predictions = torch.cat(test_predictions).cpu().numpy()\n",
        "      best_test_model_targets = torch.cat(test_targets).cpu().numpy()\n",
        "      best_test_model_accuracy = test_accuracy\n",
        "\n",
        "    torch.save(model, path + '_current_model')\n",
        "                           \n",
        "    # append history\n",
        "    history['train_losses'].append(train_loss)\n",
        "    history['train_losses_weighted'].append(train_loss_weighted)\n",
        "    history['meta_losses'].append(meta_loss)\n",
        "    history['test_losses'].append(test_loss)\n",
        "    history['train_accuracy'].append(train_accuracy)\n",
        "    history['test_accuracy'].append(test_accuracy)\n",
        "\n",
        "  # saving\n",
        "  best_model_info['model'] = best_test_model\n",
        "  best_model_info['model_predictions'] = best_test_model_predictions\n",
        "  best_model_info['test_targets'] = best_test_model_targets\n",
        "\n",
        "  torch.save({'history': history, 'model_info': best_model_info}, path + '_best_model_info')\n",
        "                           \n",
        "  # create confusion matrix\n",
        "  cm_counts_test, cm_percent_test = compute_confusion_matrix(true_targets=best_test_model_targets,\n",
        "                                                             pred_targets=best_test_model_predictions)\n",
        "  # plot and saves confusion matrix\n",
        "  plot_confusion_matrix(cm_percent_test,args,path,close=False)\n",
        "  \n",
        "  \n",
        "  # plot weight variation curves\n",
        "  if args.experiment_type == \"Uniform noise\": # ONLY FOR UNIFORM\n",
        " #   history['weight_variation_means'][0] = None\n",
        "#    history['weight_variation_stds'][0] = None\n",
        "    plot_weight_variation_curves(np.array(history['weight_variation_means']),\n",
        "                                 np.array(history['weight_variation_stds']),\n",
        "                                 args,path,close=False)\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSsRIQkkhmOt",
        "colab_type": "text"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ehotbWJhphc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_train_and_meta_loss(train_loss, meta_loss, args, path, close=True):\n",
        "    cifar_type = args.cifar_type if args.dataset == 'CIFAR' else \"\"\n",
        "    plt.figure()\n",
        "    plt.title('{}{}-{}-{}'.format(args.dataset,cifar_type,args.experiment_type,args.factor))\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.plot(train_loss, label='training loss')\n",
        "    plt.plot(meta_loss, label='meta loss')\n",
        "    plt.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "#    mng = plt.get_current_fix_manager()\n",
        "#    mng.full_screen_toggle()\n",
        "    plt.savefig(path + '_loss_plot.png')\n",
        "    if close: plt.close() \n",
        "      \n",
        "def plot_weight_variation_curves(weight_variation_means, weight_variation_stds, args, path, close=True):\n",
        "  \n",
        "    upper_bound = weight_variation_means+weight_variation_stds\n",
        "    lower_bound = weight_variation_means-weight_variation_stds\n",
        "  \n",
        "    x = np.arange(1,len(weight_variation_means)+1)\n",
        "    \n",
        "    cifar_type = args.cifar_type if args.dataset == 'CIFAR' else \"\"\n",
        "    plt.figure()\n",
        "    plt.title('{}{}-{}-{}'.format(args.dataset,cifar_type,args.experiment_type,args.factor))\n",
        "    plt.ylabel('weights')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.plot(x,weight_variation_means.tolist(), label='weight_variation', color='r')\n",
        "    plt.fill_between(x=x,\n",
        "                     y1=upper_bound,\n",
        "                     y2=lower_bound,\n",
        "                     facecolor='pink')\n",
        "    plt.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path + '_' + str(args.factor) + '_weight_variation_curve.png')\n",
        "    if close: plt.close() \n",
        "      \n",
        "def compute_confusion_matrix(true_targets,pred_targets):\n",
        "    matrix = confusion_matrix(y_true=true_targets.astype(int),\n",
        "                              y_pred=pred_targets.astype(int))\n",
        "    return matrix, matrix.astype('float') / np.sum(matrix, axis=1).reshape(-1,1)\n",
        " \n",
        "def plot_confusion_matrix(matrix,args,path,close=True):\n",
        "    cifar_type = args.cifar_type if args.dataset == 'CIFAR' else \"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(matrix, cmap='Blues', origin='lower')\n",
        "    plt.colorbar(im)\n",
        "    ax.set_title('{}-{}{}-{}-{}'.format(args.model_type,args.dataset,cifar_type,args.experiment_type,args.factor))\n",
        "    ax.set_ylabel('True label')\n",
        "    ax.set_xlabel('Predicted label')\n",
        "    ax.set_xticks(np.arange(matrix.shape[1]))\n",
        "    ax.set_yticks(np.arange(matrix.shape[0]))\n",
        "    for i in range(matrix.shape[0]):\n",
        "        for j in range(matrix.shape[1]):\n",
        "            text = ax.text(j, i, matrix[i, j],ha=\"center\", va=\"center\", color=\"r\")\n",
        "    plt.tight_layout(pad=0, w_pad=0, h_pad=0)\n",
        "    plt.get_current_fig_manager().window.showMaximized()\n",
        "    plt.savefig(path + '_confusion_matrix.png')\n",
        "    if close: plt.close()\n",
        "\n",
        "def accuracy_plot(args):\n",
        "    cifar_type = args.cifar_type if args.dataset == 'CIFAR' else \"\"\n",
        "    plt.figure()\n",
        "    plt.title('{}{}-{}-{}'.format(args.dataset,cifar_type,args.experiment_type,args.factor))\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.plot(train_loss, label='training loss')\n",
        "    plt.plot(meta_loss, label='meta loss')\n",
        "    plt.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "#    mng = plt.get_current_fix_manager()\n",
        "#    mng.full_screen_toggle()\n",
        "    plt.savefig(path + '_loss_plot.png')\n",
        "    if close: plt.close() \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uplu7oP4gLJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Args():\n",
        "    pass\n",
        "    \n",
        "args = Args()\n",
        "\n",
        "args.tau = 1e-6\n",
        "args.momentum = 0.9\n",
        "args.weight_decay = 5e-4\n",
        "args.nesterov = True\n",
        "\n",
        "args.dataset = 'CIFAR' # \"Clothing 1M\"\n",
        "args.cifar_type = 10 # 100\n",
        "args.experiment_type = \"Imbalance\" # \"Uniform noise\" # \"Flip noise\"\n",
        "\n",
        "args.model_type = \"MWN\" # Baseline, BaselineFT\n",
        "args.seed = 123\n",
        "args.num_meta_per_class = 10\n",
        "args.num_of_corrupted = 10\n",
        "\n",
        "args.num_workers = 6\n",
        "args.pin_memory = True\n",
        "args.cuda = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "args.model_signature = str(datetime.now())[0:19].replace(':','.')\n",
        "\n",
        "args.factors_dict = {\n",
        "    \"Imbalance\": [200, 100, 50, 20, 10, 1],\n",
        "    \"Uniform noise\": [0, 0.4, 0.6],\n",
        "    \"Flip noise\": [0, 0.2, 0.4],\n",
        "}\n",
        "args.factor = args.factors_dict[args.experiment_type][0]\n",
        "args.factor = 200\n",
        "\n",
        "cifar_type = str(args.cifar_type) if args.dataset == 'CIFAR' else \"\"\n",
        "\n",
        "args.directory = os.path.join(\"Experiments\",\n",
        "                              args.experiment_type,\n",
        "                              args.model_signature, \n",
        "                              args.dataset + '_'.join(['', cifar_type, args.experiment_type, str(args.factor), args.model_type])\n",
        "                             )\n",
        "\n",
        "\n",
        "args.batch_size_data_dic = {'Imbalance': 100,\n",
        "                            'Uniform noise': 100,\n",
        "                            'Flip noise': 100,\n",
        "                            'Clothing 1M': 32\n",
        "                            }\n",
        "\n",
        "args.batch_size_meta_data_dic = {'Imbalance': 100,\n",
        "                                 'Uniform noise': 100,\n",
        "                                 'Flip noise': 100,\n",
        "                                 'Clothing 1M': 32\n",
        "                                 }\n",
        "  \n",
        "args.lr_model_dic = {'Imbalance': {0: 0.1, 80: 0.01, 90: 1e-3},\n",
        "                     'Uniform noise': {0: 0.1, 36: 0.01, 38: 1e-3},\n",
        "                     'Flip noise': {0: 0.1, 40: 0.01, 50: 1e-3},\n",
        "                     'Clothing 1M': {0: 0.01, 5: 0.001}\n",
        "                     }\n",
        "\n",
        "args.lr_wnet_dic = {'Imbalance': 1e-5,\n",
        "                    'Uniform noise': 1e-3,\n",
        "                    'Flip noise': 1e-3,\n",
        "                    'Clothing 1M': 1e-3\n",
        "                    }\n",
        "\n",
        "args.epochs_dic = {'Imbalance': 100,\n",
        "                   'Uniform noise': 3, #40,\n",
        "                   'Flip noise': 60,\n",
        "                   'Clothing 1M': 10\n",
        "                   }\n",
        "\n",
        "args.weight_decay_dic = {'Imbalance': 5e-4,\n",
        "                         'Uniform noise': 5e-4,\n",
        "                         'Flip noise': 5e-4,\n",
        "                         'Clothing 1M': 1e-3\n",
        "                         }\n",
        "\n",
        "\n",
        "args.lr_schedule = args.lr_model_dic[args.experiment_type]\n",
        "args.batch_size = args.batch_size_data_dic[args.experiment_type]\n",
        "\n",
        "\n",
        "kwargs_dataloader = {'batch_size': args.batch_size,\n",
        "                     'num_workers': args.num_workers,\n",
        "                     'pin_memory': args.pin_memory,\n",
        "                     'shuffle': True}\n",
        "\n",
        "kwargs_optimizer = {'lr': args.lr_model_dic[args.experiment_type][0],\n",
        "                    'momentum': args.momentum,\n",
        "                    'nesterov': True,\n",
        "                    'weight_decay': args.weight_decay_dic[args.experiment_type]}\n",
        "\n",
        "kwargs_optimizer_wnet = {'lr': args.lr_wnet_dic[args.experiment_type],\n",
        "                          'momentum': args.momentum,\n",
        "                          'nesterov': True,\n",
        "                          'weight_decay': args.weight_decay_dic[args.experiment_type]}\n",
        "\n",
        "\n",
        "\n",
        "args.transformations = {'CIFAR': {'train': train_transform, 'test': test_transform},\n",
        "                        'Clothing 1M': {'train': None, 'test': None}}\n",
        "\n",
        "args.get_dataset_function_dict = {'CIFAR': get_CIFAR_data, 'Clothing 1M': None}\n",
        "\n",
        "train_data, test_data = args.get_dataset_function_dict[args.dataset](args.cifar_type,\n",
        "                                                                 train_transform=args.transformations[args.dataset]['train'],\n",
        "                                                                 test_transform=args.transformations[args.dataset]['test'])\n",
        "\n",
        "kwargs_data_functions = {'dataset': train_data,\n",
        "                         'factor': args.factor,\n",
        "                         'num_meta_per_class': args.num_meta_per_class,\n",
        "                         'num_of_corrupted': args.num_of_corrupted,\n",
        "                         'seed': args.seed}\n",
        "\n",
        "\n",
        "args.data_function = {'Imbalance': generate_imbalance_data,\n",
        "                       'Uniform noise': generate_noise_data,\n",
        "                       'Flip noise': generate_flip_data,\n",
        "                       'Clothing 1M': None\n",
        "                       }\n",
        "\n",
        "\n",
        "data, meta, corrupted_data = args.data_function[args.experiment_type](**kwargs_data_functions)\n",
        "\n",
        "\n",
        "#cifar_type = str(args.cifar_type) if args.dataset == 'CIFAR' else \"\"\n",
        "#data_directory = os.path.join('Data',args.experiment_type + '_' + str(args.factor) \\\n",
        "#                                      + '_' + args.dataset + cifar_type)\n",
        "#data_saved = {'data': data, 'meta': meta, 'corrupted_data': corrupted_data}\n",
        "#torch.save(data_saved, data_directory)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGObvzBQwFM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S30khCv-8vfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for path in [\"Experiments\",\n",
        "             os.path.join(\"Experiments\", args.experiment_type),\n",
        "             os.path.join(\"Experiments\", args.experiment_type, args.model_signature),\n",
        "             os.path.join(\"Experiments\", args.experiment_type, args.model_signature),\n",
        "                         args.dataset + '_'.join(['', cifar_type, args.experiment_type, str(args.factor), args.model_type])]:\n",
        "  if not os.path.exists(path):\n",
        "      os.mkdir(path)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GX9pQON1wf4",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk2xmdcW1xkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "outputId": "ec5021ec-7dcf-4899-b246-ef1c696e0184"
      },
      "source": [
        "train_loader = DataLoader(data, **kwargs_dataloader)\n",
        "meta_loader = DataLoader(meta, **kwargs_dataloader)\n",
        "test_loader = DataLoader(test_data, **kwargs_dataloader)\n",
        "corrupted_data_loader = DataLoader(corrupted_data, **kwargs_dataloader) if args.experiment_type != 'Imbalance' else None\n",
        "\n",
        "meta_weight_net = MLP().to(args.cuda)\n",
        "model = resnet32().to(args.cuda)\n",
        "meta_model = resnet32().to(args.cuda)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.backends.cudnn.benchmark = True\n",
        "\n",
        "optimizers = {}\n",
        "optimizers['model'] = torch.optim.SGD(model.parameters(), **kwargs_optimizer)\n",
        "optimizers['meta_model'] = torch.optim.SGD(meta_model.parameters(), **kwargs_optimizer)\n",
        "optimizers['meta_weight_net'] = torch.optim.SGD(meta_weight_net.parameters(), **kwargs_optimizer_wnet)\n",
        "\n",
        "loss_functions = {}\n",
        "loss_functions['model'] = nn.CrossEntropyLoss(reduction='none').to(args.cuda)\n",
        "\n",
        "\n",
        "perform_experiment(args,\n",
        "                   train_loader,\n",
        "                   meta_loader,\n",
        "                   test_loader,\n",
        "                   corrupted_data_loader,\n",
        "                   model,\n",
        "                   meta_model,\n",
        "                   meta_weight_net,\n",
        "                   optimizers,\n",
        "                   loss_functions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df70696522bd42e9a364dac99def402d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTime for epoch: 143.80621027946472, Train loss: 1.1875399351119995, Train loss weighted: 0.01191, Meta loss: 3.5492098331451416, Test loss: 3.335899829864502:   0%|          | 0/100 [02:23<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "Time for epoch: 143.80621027946472, Train loss: 1.1875399351119995, Train loss weighted: 0.01191, Meta loss: 3.5492098331451416, Test loss: 3.335899829864502:   1%|          | 1/100 [02:24<3:57:43, 144.08s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bdb6351ed2245bc9ca31e6b6e730d14",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch: 141.6654007434845, Train loss: 0.8935399651527405, Train loss weighted: 0.00896, Meta loss: 3.166529893875122, Test loss: 3.122619867324829:   2%|▏         | 2/100 [04:46<3:54:18, 143.45s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69e8af1676f24020a15a676b600ffd04",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch: 141.84407782554626, Train loss: 0.8090800046920776, Train loss weighted: 0.00811, Meta loss: 3.0084099769592285, Test loss: 2.962479829788208:   3%|▎         | 3/100 [07:08<3:51:17, 143.06s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f456c4d186964c679570c95dc44a4447",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch: 142.60373425483704, Train loss: 0.7378799915313721, Train loss weighted: 0.0074, Meta loss: 2.9455599784851074, Test loss: 2.9718098640441895:   4%|▍         | 4/100 [09:31<3:48:48, 143.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5bfdc3a2f0c4a7e905be8aacc217219",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch: 142.62056756019592, Train loss: 0.7081699967384338, Train loss weighted: 0.0071, Meta loss: 2.8955700397491455, Test loss: 2.8141398429870605:   5%|▌         | 5/100 [11:53<3:46:22, 142.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53464e08dd2f415fa88adcb0b6ec9f6e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch: 142.95960569381714, Train loss: 0.6564399600028992, Train loss weighted: 0.00658, Meta loss: 2.8811798095703125, Test loss: 2.7838199138641357:   6%|▌         | 6/100 [14:17<3:44:07, 143.06s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "505ee03551e546838dd0152d7fd97905",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch: 143.09150552749634, Train loss: 0.6150699853897095, Train loss weighted: 0.00616, Meta loss: 2.8269898891448975, Test loss: 2.7498600482940674:   7%|▋         | 7/100 [16:40<3:41:52, 143.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bda8c1bcd5a4ae7b3d6d1fbacfc2a7e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch: 143.8964638710022, Train loss: 0.5797100067138672, Train loss weighted: 0.00581, Meta loss: 2.828739881515503, Test loss: 2.7627599239349365:   8%|▊         | 8/100 [19:04<3:39:59, 143.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32f4586fba174c96844b1ad91218bdda",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch: 143.96771812438965, Train loss: 0.553380012512207, Train loss weighted: 0.00555, Meta loss: 2.73799991607666, Test loss: 2.7177999019622803:   9%|▉         | 9/100 [21:29<3:37:56, 143.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "addec0528b5244f78894914afe3ed865",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch: 144.48211812973022, Train loss: 0.5212000012397766, Train loss weighted: 0.00522, Meta loss: 2.744349956512451, Test loss: 2.649250030517578:  10%|█         | 10/100 [23:53<3:36:02, 144.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73d77e2dde8c45e39dabf15c7f921848",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch: 144.18522024154663, Train loss: 0.4995799958705902, Train loss weighted: 0.00501, Meta loss: 2.6456298828125, Test loss: 2.777289867401123:  11%|█         | 11/100 [26:18<3:33:49, 144.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2571dca99aa4b478d8108fc9a408019",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch: 143.3168032169342, Train loss: 0.47224998474121094, Train loss weighted: 0.00474, Meta loss: 2.6336898803710938, Test loss: 2.7408499717712402:  12%|█▏        | 12/100 [28:41<3:31:11, 144.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aecfee7241b64645a51deb124088711f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch: 144.6319179534912, Train loss: 0.45965999364852905, Train loss weighted: 0.00461, Meta loss: 2.612839937210083, Test loss: 2.568039894104004:  13%|█▎        | 13/100 [31:06<3:29:12, 144.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b121781ab1d4b73bfd2712899c36e06",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch: 146.90901398658752, Train loss: 0.4330099821090698, Train loss weighted: 0.00434, Meta loss: 2.5831098556518555, Test loss: 2.615769863128662:  14%|█▍        | 14/100 [33:34<3:28:04, 145.16s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5409ee0dbea949b185717df477270c32",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch: 146.66781067848206, Train loss: 0.4293999969959259, Train loss weighted: 0.0043, Meta loss: 2.549099922180176, Test loss: 2.4760499000549316:  15%|█▌        | 15/100 [36:01<3:26:24, 145.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19e8813328bf4a498464f504d2b22e3d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch: 147.38180446624756, Train loss: 0.39860999584198, Train loss weighted: 0.004, Meta loss: 2.5232598781585693, Test loss: 2.5995800495147705:  16%|█▌        | 16/100 [38:28<3:24:50, 146.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74435a1c5fc64125a8da01a887a27168",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch: 147.64171075820923, Train loss: 0.3958899974822998, Train loss weighted: 0.00397, Meta loss: 2.514240026473999, Test loss: 2.5803699493408203:  17%|█▋        | 17/100 [40:56<3:23:04, 146.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26fcaf20cbde4ba795dd592b1d513224",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2lfbbnDBGjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corrupted_data.targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5AASfaBvXwt",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szRjklpBv23X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.memory_allocated()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9xuBHDPKvyo",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KLVS3_CwAwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.max_memory_allocated()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4HNj-Om9K6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_confusion_matrix(true_targets,pred_targets):\n",
        "    matrix = confusion_matrix(y_true=true_targets.astype(int),\n",
        "                              y_pred=pred_targets.astype(int))\n",
        "    return matrix, matrix.astype('float') / np.sum(matrix, axis=1).reshape(-1,1)\n",
        " \n",
        "def plot_confusion_matrix(matrix,args,path,close=True):\n",
        "    cifar_type = args.cifar_type if args.dataset == 'CIFAR' else \"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(matrix, cmap='Blues', origin='lower')\n",
        "    plt.colorbar(im)\n",
        "    ax.set_title('{}-{}{}-{}-{}'.format(args.model_type,args.dataset,cifar_type,args.experiment_type,args.factor))\n",
        "    ax.set_ylabel('True label')\n",
        "    ax.set_xlabel('Predicted label')\n",
        "    ax.set_xticks(np.arange(matrix.shape[1]))\n",
        "    ax.set_yticks(np.arange(matrix.shape[0]))\n",
        "    for i in range(matrix.shape[0]):\n",
        "        for j in range(matrix.shape[1]):\n",
        "            text = ax.text(j, i, matrix[i, j],ha=\"center\", va=\"center\", color=\"r\")\n",
        "    #mng = plt.get_current_fig_manager()\n",
        "    #mng.full_screen_toggle()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path + '_confusion_matrix.png')\n",
        "    if close: plt.close()\n",
        "      \n",
        "\n",
        "targets = np.array([0,1,2,3,4,5,6,7,8,9,2,3,4,5,6])\n",
        "pred    = np.array([0,1,2,3,5,4,3,6,7,8,9,1,2,3,6])\n",
        "\n",
        "cm, cm_rel = compute_confusion_matrix(targets, pred)\n",
        "plot_confusion_matrix(cm_rel,args,path,close=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q27ZBWUKphn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVDgtm75v6yH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "plot_weight_variation_curves([0.05,0.07,0.09],[0.01,0.02,0.021],args,args.directory,close=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2CVStVb6xCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}